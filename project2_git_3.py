import streamlit as st
import pandas as pd
import numpy as np
import pickle
import joblib
import re
from underthesea import word_tokenize, pos_tag
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from scipy.sparse import csr_matrix, hstack
from datetime import datetime
from text_resources import load_teen_dict, load_stopwords
import plotly.express as px
import textwrap

# ==========================================================
# 1. CACHED LOADERS
# ==========================================================

@st.cache_resource
def get_resources():
    teen_dict = load_teen_dict()
    stop_words = load_stopwords()
    return teen_dict, stop_words

teen_dict, stop_words = get_resources()

def load_models():

    vectorizer = joblib.load("tfidf_vectorizer.pkl")

    with open('tfidf_matrix.pkl', 'rb') as f:
        tfidf_matrix = pickle.load(f)

    with open("kmeans.pkl", "rb") as f:
        kmeans = pickle.load(f)

    with open("scaler.pkl", "rb") as f:
        scaler = pickle.load(f)

    ohe = joblib.load("onehot_encoder.pkl")

    imputer = joblib.load("imputer.pkl")

    with open("pca.pkl", "rb") as f:
        pca = pickle.load(f)

    return vectorizer, tfidf_matrix, kmeans, scaler, ohe, imputer, pca


@st.cache_data
def compute_clusters(df_cluster):
    # models are accessed from global scope:
    global scaler, kmeans, pca

    num_cols = ['age', 'mileage_km', 'min_price', 'max_price', 'log_price']

    X_scaled = scaler.transform(df_cluster[num_cols])
    df_cluster['cluster_label'] = kmeans.predict(X_scaled)

    pca_points = pca.transform(X_scaled)
    df_cluster['x'] = pca_points[:, 0]
    df_cluster['y'] = pca_points[:, 1]

    return df_cluster, num_cols

def load_raw_data():
    data = pd.read_excel('data_motobikes.xlsx').rename(columns={
        'Ti√™u ƒë·ªÅ': 'title',
        'ƒê·ªãa ch·ªâ': 'address',
        'M√¥ t·∫£ chi ti·∫øt': 'description',
        'Gi√°': 'price',
        'Kho·∫£ng gi√° min': 'min_price',
        'Kho·∫£ng gi√° max': 'max_price',
        'Th∆∞∆°ng hi·ªáu': 'brand',
        'D√≤ng xe': 'model',
        'NƒÉm ƒëƒÉng k√Ω': 'registration_year',
        'S·ªë Km ƒë√£ ƒëi': 'mileage_km',
        'T√¨nh tr·∫°ng': 'condition',
        'Lo·∫°i xe': 'bike_type',
        'Dung t√≠ch xe': 'engine_capacity',
        'Xu·∫•t x·ª©': 'origin',
        'Ch√≠nh s√°ch b·∫£o h√†nh': 'warranty_policy',
        'Tr·ªçng l∆∞·ª£ng': 'weight'
    })
    return data

def clean_text(text): # t·∫°o h√†m x·ª≠ l√Ω text v·ªõi text l√† chu·ªói c√°c t·ª´

    text = str(text).lower()
    text = text.replace('\n', ' ')
    text = re.sub(r'[^a-zA-Z√Ä-·ªπ0-9\s]', '', text)
    text = re.sub(r'\b\w\b', '', text)

    # Teen-code normalization
    words = text.split()
    words = [teen_dict.get(w, w) for w in words]
    text = ' '.join(words)

    # Tokenize & POS filter
    tokenized = word_tokenize(text)
    pos_tagged_text = pos_tag(" ".join(tokenized))
    filtered_words = [word for word, tag in pos_tagged_text if tag != 'T']

    # Stopword removal
    clean_words = [word for word in filtered_words if word not in stop_words]

    # Return string (not list), same as df['content_clean_cosine']
    return " ".join(clean_words)

def clean_df_for_recommender(df):
    ### For numeric part of vector

    # clean price
    df['price'] = (
    df['price']
    .astype(str)
    .str.replace('[^0-9]', '', regex=True)   # ch·ªâ gi·ªØ l·∫°i ch·ªØ s·ªë
    .replace('', np.nan)
    .astype(float)
)
    def parse_minmax_price(s):
        if pd.isna(s):
            return np.nan
        s = str(s).lower().replace("tr", "").replace(" ", "")
        try:
            return float(s) * 1_000_000
        except:
            return np.nan

    df['min_price'] = df['min_price'].apply(parse_minmax_price)
    df['max_price'] = df['max_price'].apply(parse_minmax_price)

    # X√°c ƒë·ªãnh num/ non-num cols ƒë·ªÉ fill NA
    num_cols = df.select_dtypes(include=["int64", "float64"]).columns
    cat_cols = df.select_dtypes(include=["object"]).columns

    # Fill NA (num -> median, non-num -> mode)
    # 1. Numeric imputation
    num_imputer = joblib.load('imputer.pkl')
    df[num_cols] = num_imputer.fit_transform(df[num_cols])

    # 2. Categorical imputation
    cat_imputer = SimpleImputer(strategy="most_frequent")
    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

    # Thay th·∫ø c√°c gi√° tr·ªã kh√¥ng r√µ trong c·ªôt 'engine_capacity'
    df['engine_capacity'] = df['engine_capacity'].replace(
        ['Kh√¥ng bi·∫øt r√µ', 'ƒêang c·∫≠p nh·∫≠t', 'Nh·∫≠t B·∫£n'],
        'Unknown'
    )

    # Thay th·∫ø c√°c gi√° tr·ªã kh√¥ng r√µ trong c·ªôt 'origin', gi·ªØ nguy√™n nh√≥m "B·∫£o h√†nh h√£ng" ƒë·ªÉ x·ª≠ l√Ω text
    df['origin'] = df['origin'].replace(
        ['ƒêang c·∫≠p nh·∫≠t', 'N∆∞·ªõc kh√°c'],
        'N∆∞·ªõc kh√°c'
    )

    # Chu·∫©n h√≥a registration_year
    df['registration_year'] = (
        df['registration_year']
        .astype(str)
        .str.lower()
        .str.replace('tr∆∞·ªõc nƒÉm', '1980', regex=False)
        .str.extract('(\d{4})')[0]
    )
    # Chuy·ªÉn sang numeric, nh·ªØng gi√° tr·ªã kh√¥ng chuy·ªÉn ƒë∆∞·ª£c s·∫Ω th√†nh NA
    df['registration_year'] = pd.to_numeric(df['registration_year'], errors='coerce')

    # Fill NA ban ƒë·∫ßu
    df['registration_year'] = df['registration_year'].fillna(df['registration_year'].median())

    # G·∫Øn gi√° tr·ªã b·∫•t h·ª£p l·ªá th√†nh NA
    df.loc[
        (df['registration_year'] < 1980) | (df['registration_year'] > 2025),
        'registration_year'
    ] = np.nan

    # Fill NA sau khi lo·∫°i b·∫•t h·ª£p l·ªá
    df['registration_year'] = df['registration_year'].fillna(df['registration_year'].median())

    # Th√™m bi·∫øn age
    current_year = datetime.now().year
    df['age'] = current_year - df['registration_year']

    # gom nh√≥m brand hi·∫øm v√† t·∫°o c·ªôt 'segment'
    brand_counts = df['brand'].value_counts()
    rare_brands = brand_counts[brand_counts < 50].index
    df['brand_grouped'] = df['brand'].replace(rare_brands, 'H√£ng kh√°c')

    def group_model(x):
        counts = x.value_counts()
        rare_models = counts[counts < 100].index
        return x.replace(rare_models, 'D√≤ng kh√°c')

    df['model_grouped'] = df.groupby('brand_grouped')['model'].transform(group_model)
    df['segment'] = df['brand_grouped'] + '_' + df['model_grouped']

    # One hot encoding 'bike_type', 'engine_capacity'
    encoded = ohe.transform(df[['bike_type', 'engine_capacity']])
    encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(['bike_type', 'engine_capacity']))
    # merge back to original dataframe
    df = pd.concat([df, encoded_df], axis=1)

    # numeric features
    num_features = ['price','mileage_km','min_price','max_price','age', 'registration_year']
    # log normalize numeric features
    normalized_features = []
    for col in num_features:
        new_col = col + "_log"
        df[new_col] = np.log1p(df[col])
        normalized_features.append(new_col)

    # t·∫°o feature brand_meanprice
    brand_mean_log = df.groupby('brand')['price_log'].mean().rename('brand_meanprice')
    df = df.merge(brand_mean_log, on='brand', how='left')
    normalized_features.append('brand_meanprice')

    # features to turn to a vector: 
    onehot_features = ohe.get_feature_names_out(['bike_type', 'engine_capacity']).tolist()
    num_features = onehot_features + normalized_features

    # X·ª≠ l√Ω NaN (n·∫øu c√≥) ƒë·ªÉ t·∫°o dense vector cho vi·ªác t√≠nh to√°n cosine similarity l√∫c sau
    X_num = df[num_features].copy()

    # 1Ô∏è‚É£ Impute missing values
    # imputer = SimpleImputer(strategy="median")
    X_num_imputed = imputer.fit_transform(X_num)

    # 2Ô∏è‚É£ Scaling for num features
    scaler = StandardScaler()
    X_num_scaled = scaler.fit_transform(X_num_imputed)

    ### For text part of vector
    # ·ªû ƒë√¢y ƒë√£ load tfidf_matrix n√™n kh√¥ng x·ª≠ l√Ω ph·∫ßn text n·ªØa

    ### T·∫°o vector ƒë·∫ßu v√†o b·∫±ng c√°ch k·∫øt h·ª£p vector TF-IDF v√† array num col (X_num_scaled)
    # from scipy.sparse import csr_matrix, hstack
    # Chuy·ªÉn array X_num_scaled th√†nh matrix d·∫°ng sparse (ko store c√°c gi√° tr·ªã 0)
    X_num_sparse = csr_matrix(X_num_scaled)

    # Gh√©p ma tr·∫≠n TF-IDF v√† ma tr·∫≠n X_num_sparse theo chi·ªÅu ngang
    X_final = hstack([tfidf_matrix, X_num_sparse])

    return df, X_final

def clean_df_for_clustering(df_cluster):
    cols_drop = ['title', 'address', 'description', 'Href']
    df_cluster = df_cluster.drop(columns=[c for c in cols_drop if c in df_cluster.columns], errors='ignore')
    df_cluster = df_cluster.drop(columns=['warranty_policy', 'weight', 'condition'], errors='ignore')
    df_cluster = df_cluster.dropna()

    # Clean price
    df_cluster['price'] = (
        df_cluster['price'].astype(str)
        .str.replace('[^0-9]', '', regex=True)
        .replace('', np.nan).astype(float)
    )

    # Minimal cleaning df price for display
    if 'price' in df_cluster.columns:
        df_cluster['price'] = df_cluster['price'].astype(str).str.replace('[^0-9]', '', regex=True)
        df_cluster.loc[df_cluster['price'] == '', 'price'] = np.nan
        df_cluster['price'] = pd.to_numeric(df_cluster['price'], errors='coerce')

    # ensure registration_year numeric
    if 'registration_year' in df_cluster.columns:
        df_cluster['registration_year'] = (
            df_cluster['registration_year'].astype(str)
            .str.lower()
            .str.replace('tr∆∞·ªõc nƒÉm', '1980', regex=False)
            .str.extract(r'(\d{4})')[0]
        )
        df_cluster['registration_year'] = pd.to_numeric(df_cluster['registration_year'], errors='coerce')
        df_cluster.loc[(df_cluster['registration_year'] < 1980) | (df_cluster['registration_year'] > 2025), 'registration_year'] = np.nan
    
    def parse_price(s):
        if pd.isna(s): return np.nan
        s = str(s).lower().replace("tr", "").replace(" ", "")
        try: return float(s) * 1_000_000
        except: return np.nan

    df_cluster['min_price'] = df_cluster['min_price'].apply(parse_price)
    df_cluster['max_price'] = df_cluster['max_price'].apply(parse_price)

    df_cluster = df_cluster[~(df_cluster['price'] == 0)]

    # Remove invalid engine_capacity
    df_cluster = df_cluster[~df_cluster['engine_capacity'].astype(str).str.contains("Nh·∫≠t B·∫£n", na=False)]

    # Clean origin
    df_cluster = df_cluster[~df_cluster['origin'].astype(str).str.contains('B·∫£o h√†nh h√£ng', case=False, na=False)]
    df_cluster['origin'] = df_cluster['origin'].replace(['ƒêang c·∫≠p nh·∫≠t', 'N∆∞·ªõc kh√°c'], 'N∆∞·ªõc kh√°c')

    # Registration year
    df_cluster['registration_year'] = (
        df_cluster['registration_year'].astype(str)
        .str.lower()
        .str.replace('tr∆∞·ªõc nƒÉm', '1980')
        .str.extract('(\d{4})')[0]
    ).astype(float)

    df_cluster.loc[(df_cluster['registration_year'] < 1980) | (df_cluster['registration_year'] > 2025),
            'registration_year'] = np.nan

    df_cluster["age"] = 2025 - df_cluster["registration_year"]

    # Log transforms
    numeric_cols = ['age', 'mileage_km', 'min_price', 'max_price', 'price']
    for c in numeric_cols:
        df_cluster[f"log_{c}"] = np.log1p(df_cluster[c])

    df_cluster = df_cluster.dropna(subset=numeric_cols)

    return df_cluster



# ==========================================================
# LOAD EVERYTHING (CACHED)
# ==========================================================
# 1) Load models
vectorizer, tfidf_matrix, kmeans, scaler, ohe, imputer, pca = load_models()

# 2) Load raw data
df_raw = load_raw_data()

# 3) Prepare recommender dataset
df_clean, X_final = clean_df_for_recommender(df_raw.copy())

# 4) Prepare clustering dataset
df_cluster = clean_df_for_clustering(df_raw.copy())
df_cluster, num_cols = compute_clusters(df_cluster)


# ==========================================================
# FUNCTIONS
# ==========================================================
def preprocess_user_input(price, min_price, max_price, mileage_km, registration_year):
    age = 2025 - registration_year
    log_price = np.log1p(price)
    X = np.array([[age, mileage_km, min_price, max_price, log_price]])
    return scaler.transform(X)

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def get_top_n_similar_by_content(df, X_final, title, top_n=5):
    """
    Given a bike title, return top N most similar bikes based on
    combined TF-IDF + numeric features vector.

    Params:
        df (DataFrame): cleaned dataframe returned from clean_df_for_recommender
        X_final (sparse matrix): combined feature matrix
        title (str): the selected bike title
        top_n (int): number of similar bikes to return

    Returns:
        df_recommend (DataFrame): rows of top-N similar bikes
        scores (list): similarity scores
    """

    # 1Ô∏è‚É£ Find the index of the selected bike
    matches = df.index[df['title'] == title]

    if len(matches) == 0:
        return None, []   # title not found

    idx = matches[0]

    # 2Ô∏è‚É£ Compute cosine similarity for this single item
    sims = cosine_similarity(X_final[idx], X_final).flatten()

    # 3Ô∏è‚É£ Sort by similarity (descending), ignore itself
    ranked_indices = np.argsort(sims)[::-1]

    # Remove itself
    ranked_indices = ranked_indices[ranked_indices != idx]

    # 4Ô∏è‚É£ Take top-N
    top_indices = ranked_indices[:top_n]
    top_scores = sims[top_indices]

    # 5Ô∏è‚É£ Return matching rows + scores
    df_recommend = df.iloc[top_indices].copy()
    df_recommend['similarity_score'] = top_scores

    return df_recommend, top_scores.tolist()

# helper: safe format number
def fmt_vnd(x):
    try:
        return f"{int(x):,} VNƒê"
    except:
        return '-'


# ==========================================================
# STREAMLIT PAGES
# ==========================================================
st.set_page_config(
    page_title="H·ªá th·ªëng g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± v√† ph√¢n c·ª•m xe m√°y",
    page_icon="üèçÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.sidebar.markdown("""
## H·ªá th·ªëng g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± v√† ph√¢n c·ª•m xe m√°y
""")

st.sidebar.markdown("""
### Th√†nh vi√™n nh√≥m 6
1. V≈© Th·ªã Ng·ªçc Anh
2. Nguy·ªÖn Ph·∫°m Qu·ª≥nh Anh
""")

st.sidebar.markdown("### Menu")   
menu = ["Gi·ªõi thi·ªáu", "B√†i to√°n nghi·ªáp v·ª•", "ƒê√°nh gi√° m√¥ h√¨nh v√† B√°o c√°o",
        "G·ª£i √Ω m·∫´u xe t∆∞∆°ng t·ª±", "Ph√¢n c·ª•m ph√¢n kh√∫c xe m√°y"]
page = st.sidebar.selectbox("", menu)  


# ==========================================================
# STYLES
# ==========================================================

BASE_CSS = """
<style>
:root{
  --accent-1: #ffde37;       /* Your yellow */
  --accent-2: #e5c620;       /* Slightly darker yellow for gradients */
  --muted: #4a4a4a;
  --card-bg: #fff7c2;        /* Soft light yellow background */
  --glass: rgba(255,255,255,0.55);
}

/* Background */
html, body {
  background: linear-gradient(180deg, #fff5a0 0%, #ffef73 100%);
  color: #000000 !important;
}

/* Header / hero section */
.header-hero {
  background: linear-gradient(90deg, var(--accent-1), var(--accent-2));
  padding: 22px;
  border-radius: 12px;
  color: #000000;
  font-weight: 600;
  margin-bottom: 18px;
  box-shadow: 0 6px 24px rgba(0,0,0,0.12);
}

/* Small muted text */
.small-muted {
  color: var(--muted);
  font-size: 13px;
}

/* Cards */
.card {
  background: var(--card-bg);
  padding: 14px;
  border-radius: 12px;
  box-shadow: 0 6px 18px rgba(0,0,0,0.10);
  color: #000000;
}

/* Base typography */
h1, h2, h3, h4, h5, h6, p, span, div {
  color: #000000 !important;
}

/* Bike title / subtitles */
.bike-title{
  font-size:18px;
  font-weight:700;
  margin-bottom:4px;
}

.bike-sub{
  font-size:13px;
  color:var(--muted);
  margin-bottom:6px;
}

/* Cluster cards */
.cluster-card{
  padding:18px;
  border-radius:12px;
  color:#000000;
  margin-bottom:12px;
  font-weight:600;
}

/* Cluster variants using your yellow palette */
.cluster-0{
  background:linear-gradient(135deg, #ffeb7a, #ffde37);
}
.cluster-1{
  background:linear-gradient(135deg, #ffe45c, #e5c620);
}
.cluster-2{
  background:linear-gradient(135deg, #fff1a1, #ffde37);
}
</style>
"""

st.markdown(BASE_CSS, unsafe_allow_html=True)


# ==========================================================
# PAGE CONTENT
# ==========================================================

if page == 'Gi·ªõi thi·ªáu':
    # st.title("H·ªá th·ªëng g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± v√† ph√¢n c·ª•m xe m√°y")
    st.markdown("""
        <h1 style='font-size:48px; font-weight:800; margin-bottom:8px;'>
            H·ªá th·ªëng g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± v√† ph√¢n c·ª•m xe m√°y
        </h1>
        <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
    """, unsafe_allow_html=True)    
    st.image("xe_may_cu2.jpg")
    st.subheader("[Trang ch·ªß Ch·ª£ T·ªët](https://www.chotot.com/)")

        # Function for light yellow pad header
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)
    
    yellow_pad_header('Gi·ªõi thi·ªáu d·ª± √°n')
    st.markdown('''ƒê√¢y l√† d·ª± √°n x√¢y d·ª±ng h·ªá th·ªëng h·ªó tr·ª£ **g·ª£i √Ω m·∫´u xe m√°y t∆∞∆°ng t·ª±** 
v√† **ph√¢n kh√∫c xe m√°y b·∫±ng ph∆∞∆°ng ph√°p ph√¢n c·ª•m** tr√™n n·ªÅn t·∫£ng *Ch·ª£ T·ªët* ‚Äì 
trong kh√≥a ƒë·ªì √°n t·ªët nghi·ªáp Data Science and Machine Learning 2024 l·ªõp DL07_K308 c·ªßa nh√≥m 6.

Th√†nh vi√™n nh√≥m g·ªìm c√≥:
1. V≈© Th·ªã Ng·ªçc Anh  
2. Nguy·ªÖn Ph·∫°m Qu·ª≥nh Anh
''')

    yellow_pad_header('M·ª•c ti√™u c·ªßa d·ª± √°n')
    st.markdown("""
    **1. X√¢y d·ª±ng m√¥ h√¨nh ƒë·ªÅ xu·∫•t th√¥ng minh:**
    - ƒê·ªÅ xu·∫•t c√°c m·∫´u xe m√°y t∆∞∆°ng ƒë·ªìng cho m·ªôt m·∫´u ƒë∆∞·ª£c ch·ªçn ho·∫∑c theo t·ª´ kh√≥a t√¨m ki·∫øm.
    - K·∫øt h·ª£p nhi·ªÅu ngu·ªìn th√¥ng tin (th√¥ng s·ªë k·ªπ thu·∫≠t, h√¨nh ·∫£nh, m√¥ t·∫£, gi√°, ƒë√°nh gi√°) ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c.

    **2. Ph√¢n kh√∫c th·ªã tr∆∞·ªùng xe m√°y:**
    - Ph√¢n lo·∫°i s·∫£n ph·∫©m theo nh√≥m theo t·ªáp gi√°, tu·ªïi xe, kho·∫£ng gi√° t·ªëi thi·ªÉu/t·ªëi ƒëa.
    - H·ªó tr·ª£ ƒë·ªãnh gi√° v√† x√¢y d·ª±ng chi·∫øn l∆∞·ª£c marketing hi·ªáu qu·∫£ h∆°n.
    """)

    yellow_pad_header('Ph√¢n c√¥ng c√¥ng vi·ªác')
    st.write("""
    - **X·ª≠ l√Ω d·ªØ li·ªáu:** Ng·ªçc Anh v√† Qu·ª≥nh Anh  
    - **G·ª£i √Ω xe m√°y b·∫±ng Gensim:** Qu·ª≥nh Anh  
    - **G·ª£i √Ω xe m√°y b·∫±ng Cosine similarity:** Ng·ªçc Anh  
    - **Ph√¢n kh√∫c xe m√°y b·∫±ng ph∆∞∆°ng ph√°p ph√¢n c·ª•m:** Ng·ªçc Anh  
    - **L√†m slide:** Ng·ªçc Anh v√† Qu·ª≥nh Anh  
    - **Giao di·ªán Streamlit:** Qu·ª≥nh Anh
    """)
    
elif page == 'B√†i to√°n nghi·ªáp v·ª•':
    st.markdown("""
    <h1 style='font-size:48px; font-weight:800; margin-bottom:8px;'>
        B√†i to√°n nghi·ªáp v·ª•
    </h1>
    <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
""", unsafe_allow_html=True)
    # Function for light yellow pad header
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)

    yellow_pad_header('V·∫•n ƒë·ªÅ nghi·ªáp v·ª•')
    st.markdown("""
        - Ng∆∞·ªùi d√πng g·∫∑p kh√≥ khƒÉn khi t√¨m xe ph√π h·ª£p trong h√†ng trƒÉm l·ª±a ch·ªçn.
        - Ch∆∞a c√≥ h·ªá th·ªëng g·ª£i √Ω xe t∆∞∆°ng t·ª± khi ng∆∞·ªùi d√πng ch·ªçn m·ªôt m·∫´u c·ª• th·ªÉ ho·∫∑c t√¨m ki·∫øm theo t·ª´ kh√≥a.
        - Th·ªã tr∆∞·ªùng xe m√°y r·∫•t ƒëa d·∫°ng ‚Üí kh√≥ nh·∫≠n di·ªán c√°c ph√¢n kh√∫c r√µ r√†ng.
        - C·∫ßn h·ªá th·ªëng g·ª£i √Ω & ph√¢n kh√∫c t·ª± ƒë·ªông ƒë·ªÉ h·ªó tr·ª£ ng∆∞·ªùi d√πng v√† ƒë·ªôi ng≈© ph√¢n t√≠ch.""")

    yellow_pad_header('B√†i to√°n ƒë·∫∑t ra')
    st.markdown("""
        1. X√¢y d·ª±ng m√¥ h√¨nh **G·ª£i √Ω xe t∆∞∆°ng t·ª±**
        - S·ª≠ d·ª•ng c√°c ƒë·∫∑c tr∆∞ng t·ª´ m√¥ t·∫£ xe v√† th√¥ng s·ªë k·ªπ thu·∫≠t
        - G·ª£i √Ω c√°c m·∫´u xe t∆∞∆°ng t·ª± v·ªõi xe ƒë∆∞·ª£c ch·ªçn ho·∫∑c theo t·ª´ kh√≥a t√¨m ki·∫øm.
        &nbsp;
        2. X√¢y d·ª±ng m√¥ h√¨nh **Ph√¢n kh√∫c th·ªã tr∆∞·ªùng xe b·∫±ng ph∆∞∆°ng ph√°p ph√¢n c·ª•m**
        - Ph√¢n c·ª•m th·ªã tr∆∞·ªùng xe m√°y d·ª±a c√°c ƒë·∫∑c tr∆∞ng gi√° xe, tu·ªïi xe, s·ªë km ƒë√£ ch·∫°y, kho·∫£ng gi√° t·ªëi thi·ªÉu, t·ªëi ƒëa.
        - Gi√∫p nh·∫≠n di·ªán v√† ph√¢n lo·∫°i xe theo c√°c ph√¢n kh√∫c kh√°c nhau.
                """)
    
    yellow_pad_header('Ph·∫°m vi tri·ªÉn khai')
    st.markdown("""
        **1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v√† chu·∫©n h√≥a**:  
            - Chu·∫©n h√≥a c√°c th√¥ng s·ªë c·ªßa xe.  
            - L√†m s·∫°ch d·ªØ li·ªáu v√† chu·∫©n h√≥a tr∆∞·ªùng th√¥ng tin cho m√¥ h√¨nh.  
                
        **2. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n v√† t√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng**:  
            - S·ª≠ d·ª•ng **TF-IDF Vectorizer** ƒë·ªÉ m√£ h√≥a m√¥ t·∫£ v√† th√¥ng tin k·ªπ thu·∫≠t.  
            - T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng b·∫±ng **gensim similarity** v√† **cosine similarity**.  
            - Ch·ªçn ph∆∞∆°ng ph√°p cho **ƒëi·ªÉm cao h∆°n** v√† **nghƒ©a ƒë√∫ng h∆°n** ƒë·ªÉ ƒë∆∞a v√†o h·ªá th·ªëng g·ª£i √Ω.  
                
        **3. Ph√¢n c·ª•m th·ªã tr∆∞·ªùng (Clustering)**:  
            - Th·ª≠ nghi·ªám tr√™n c√°c thu·∫≠t to√°n: KMeans, Bisecting KMeans, Agglomerative Clustering  
            - ƒê√°nh gi√° b·∫±ng inertia, silhouette score, t√≠nh di·ªÖn gi·∫£i.  
            - Ch·ªçn **KMeans** v√¨ c√≥ hi·ªáu su·∫•t ·ªïn ƒë·ªãnh, d·ªÖ di·ªÖn gi·∫£i v√† ranh gi·ªõi c·ª•m ph√π h·ª£p h∆°n v·ªõi d·ªØ li·ªáu.

        **4. X√¢y d·ª±ng GUI tr√™n Streamlit**:  
            - Cho ph√©p ng∆∞·ªùi d√πng **ch·ªçn xe trong danh s√°ch** ho·∫∑c **nh·∫≠p m√¥ t·∫£ xe** ‚Üí tr·∫£ v·ªÅ **danh s√°ch m·∫´u xe t∆∞∆°ng t·ª± c√≥ trong s√†n**.  
            - Cho ph√©p **nh·∫≠p t√™n xe** ‚Üí hi·ªÉn th·ªã **xe thu·ªôc c·ª•m/ph√¢n kh√∫c n√†o**.
                """)

    yellow_pad_header('Thu th·∫≠p d·ªØ li·ªáu')
    st.markdown("""
        - B·ªô d·ªØ li·ªáu g·ªìm **7.208 tin ƒëƒÉng** v·ªõi **18 thu·ªôc t√≠nh** (th∆∞∆°ng hi·ªáu, d√≤ng xe, s·ªë km, nƒÉm ƒëƒÉng k√Ω, gi√° ni√™m y·∫øt, m√¥ t·∫£, v.v‚Ä¶) ƒë∆∞·ª£c thu th·∫≠p t·ª´ n·ªÅn t·∫£ng **Ch·ª£ T·ªët** (tr∆∞·ªõc ng√†y 01/07/2025).
        - B·ªô d·ªØ li·ªáu bao g·ªìm c√°c th√¥ng tin sau:
            - **id**: s·ªë th·ª© t·ª± c·ªßa s·∫£n ph·∫©m trong b·ªô d·ªØ li·ªáu  
            - **Ti√™u ƒë·ªÅ**: t·ª±a ƒë·ªÅ b√†i ƒëƒÉng b√°n s·∫£n ph·∫©m  
            - **Gi√°**: gi√° b√°n c·ªßa xe m√°y  
            - **Kho·∫£ng gi√° min**: gi√° s√†n ∆∞·ªõc t√≠nh c·ªßa xe m√°y  
            - **Kho·∫£ng gi√° max**: gi√° tr·∫ßn ∆∞·ªõc t√≠nh c·ªßa xe m√°y  
            - **ƒê·ªãa ch·ªâ**: ƒë·ªãa ch·ªâ giao d·ªãch (ph∆∞·ªùng, qu·∫≠n, th√†nh ph·ªë H·ªì Ch√≠ Minh)  
            - **M√¥ t·∫£ chi ti·∫øt**: m√¥ t·∫£ th√™m v·ªÅ s·∫£n ph·∫©m ‚Äî ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t, t√¨nh tr·∫°ng, th√¥ng tin kh√°c  
            - **Th∆∞∆°ng hi·ªáu**: h√£ng s·∫£n xu·∫•t (Honda, Yamaha, Piaggio, SYM‚Ä¶)  
            - **D√≤ng xe**: d√≤ng xe c·ª• th·ªÉ (Air Blade, Vespa, Exciter, LEAD, Vario, ‚Ä¶)  
            - **NƒÉm ƒëƒÉng k√Ω**: nƒÉm ƒëƒÉng k√Ω l·∫ßn ƒë·∫ßu c·ªßa xe  
            - **S·ªë km ƒë√£ ƒëi**: s·ªë kilomet xe ƒë√£ v·∫≠n h√†nh  
            - **T√¨nh tr·∫°ng**: t√¨nh tr·∫°ng hi·ªán t·∫°i (v√≠ d·ª•: ƒë√£ s·ª≠ d·ª•ng)  
            - **Lo·∫°i xe**: Xe s·ªë, Tay ga, Tay c√¥n/Moto  
            - **Dung t√≠ch xe**: dung t√≠ch xi-lanh (v√≠ d·ª•: D∆∞·ªõi 50cc, 50‚Äì100cc, 100‚Äì175cc, ‚Ä¶)  
            - **Xu·∫•t x·ª©**: qu·ªëc gia s·∫£n xu·∫•t (Vi·ªát Nam, ƒê√†i Loan, Nh·∫≠t B·∫£n, ...)  
            - **Ch√≠nh s√°ch b·∫£o h√†nh**: th√¥ng tin b·∫£o h√†nh n·∫øu c√≥  
            - **Tr·ªçng l∆∞·ª£ng**: tr·ªçng l∆∞·ª£ng ∆∞·ªõc t√≠nh c·ªßa xe  
            - **Href**: ƒë∆∞·ªùng d·∫´n t·ªõi b√†i ƒëƒÉng s·∫£n ph·∫©m 
                """)

elif page == 'ƒê√°nh gi√° m√¥ h√¨nh v√† B√°o c√°o':
    st.markdown("""
    <h1 style='font-size:48px; font-weight:800; margin-bottom:8px;'>
        ƒê√°nh gi√° m√¥ h√¨nh v√† B√°o c√°o
    </h1>
    <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
""", unsafe_allow_html=True)
    
    # Function for light yellow pad header
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True) 

    yellow_pad_header('Th·ªëng k√™ m√¥ t·∫£ s∆° b·ªô')


    st.markdown("""        
    B·ªô d·ªØ li·ªáu g·ªìm **7.208 tin ƒëƒÉng** v·ªõi **18 thu·ªôc t√≠nh** (th∆∞∆°ng hi·ªáu, d√≤ng xe, s·ªë km, nƒÉm ƒëƒÉng k√Ω, gi√° ni√™m y·∫øt, m√¥ t·∫£‚Ä¶) ƒë∆∞·ª£c thu th·∫≠p t·ª´ n·ªÅn t·∫£ng **Ch·ª£ T·ªët** (tr∆∞·ªõc ng√†y 01/07/2025).  
                """)

    # Hi·ªÉn th·ªã 4 bi·ªÉu ƒë·ªì d·∫°ng l∆∞·ªõi 2x2
    col1, col2 = st.columns(2)
    with col1:
        st.image("brand_grouped_count.png")
        st.image("age_bin_stats.png")

    with col2:
        st.image("price_bin_stats.png")
        st.image("mileage_bin_stats.png")

    yellow_pad_header('M√¥ h√¨nh g·ª£i √Ω xe m√°y t∆∞∆°ng t·ª±')

    # with open("data/data_motobikes.xlsx", "rb") as f:
    #     st.download_button(
    #         label="üì• T·∫£i xu·ªëng d·ªØ li·ªáu xe m√°y (Excel)",
    #         data=f,
    #         file_name="data_motobikes.xlsx",
    #         mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    #     )

    st.markdown('#### 1. H∆∞·ªõng x·ª≠ l√Ω')
    st.write('''
             - Chu·∫©n h√≥a v√† l√†m s·∫°ch d·ªØ li·ªáu.
             - Chia kho·∫£ng m·ªôt s·ªë ƒë·∫∑c tr∆∞ng ki·ªÉu s·ªë ƒë·ªÉ t·∫°o th√™m c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i m·ªõi (kho·∫£ng gi√°, t√¨nh tr·∫°ng d·ª±a theo s·ªë km ch·∫°y, tu·ªïi xe, dung t√≠ch xe)
             - Gom c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i th√†nh bi·∫øn text
             - L√†m s·∫°ch text v√† tokenize, x√¢y d·ª±ng ma tr·∫≠n t∆∞∆°ng ƒë·ªìng (sparse matrix) gi·ªØa c√°c vƒÉn b·∫£n ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô gi·ªëng nhau
             - T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng b·∫±ng gensim v√† cosine similarity
                 - Tr∆∞·ªùng h·ª£p 1: g·ª£i √Ω xe theo id s·∫£n ph·∫©m ƒë∆∞·ª£c ch·ªçn
                    - Ng∆∞·ªùi d√πng ch·ªçn xe t·ª´ danh s√°ch xe trong t·∫≠p d·ªØ li·ªáu
                    - D·ª±a tr√™n ma tr·∫≠n t∆∞∆°ng ƒë·ªìng, t√¨m c√°c xe c√≥ similarity score cao nh·∫•t.
                    - T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng trung b√¨nh gi·ªØa 5 m·∫´u g·ª£i √Ω cho m·ªôt m·∫´u, sau ƒë√≥ √°p d·ª•ng cho 7000 m·∫´u trong t·∫≠p d·ªØ li·ªáu v√† t√≠nh trung b√¨nh.

                 - Tr∆∞·ªùng h·ª£p 2: g·ª£i √Ω xe theo c·ª•m t·ª´ kh√≥a t√¨m ki·ªÉm (vd: ‚Äúhonda vision xanh d∆∞·ªõi 15 tri·ªáu‚Äù)
                    - Ng∆∞·ªùi d√πng nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm. 
                    - X·ª≠ l√Ω t·ª´ kh√≥a v√† chuy·ªÉn t·ª´ kh√≥a th√†nh vector s·ªë d·ª±a tr√™n t·ª´ ƒëi·ªÉn v√† TF-IDF
                    - T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa t·ª´ kh√≥a v√† t·∫•t c·∫£ xe trong d·ªØ li·ªáu. 
                    - S·∫Øp x·∫øp v√† l·∫•y ra 5 xe g·ª£i √Ω ph√π h·ª£p nh·∫•t.
                    - Cho danh s√°ch 10 c·ª•m t·ª´ kh√≥a t√¨m ki·∫øm. T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng trung b√¨nh gi·ªØa 5 m·∫´u g·ª£i √Ω cho m·ªôt m·∫´u, sau ƒë√≥ √°p d·ª•ng cho 10 c·ª•m t·ª´ tr√™n v√† t√≠nh trung b√¨nh
             ''')
    
    st.markdown('#### 2. K·∫øt qu·∫£')
    st.write('Gi·ªØa 02 m√¥ h√¨nh Gensim v√† Cosine similarity, Cosine similarity, trong c·∫£ 2 tr∆∞·ªùng h·ª£p ch·ªçn xe c√≥ s·∫µn ho·∫∑c t√¨m b·∫±ng t·ª´ kh√≥a, cho ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng trung b√¨nh cao h∆°n so v·ªõi Gensim v√† cho c√°c g·ª£i √Ω s√°t nghƒ©a h∆°n Gensim.\nM√¥ h√¨nh d√πng ƒë·ªÉ d·ª± ƒëo√°n xe trong ·ª©ng d·ª•ng n√†y l√† Cosine similarity.') 

    yellow_pad_header('M√¥ h√¨nh ph√¢n kh√∫c xe m√°y')
    
    st.markdown('#### 1. X·ª≠ l√Ω d·ªØ li·ªáu')
    st.write('D·ªØ li·ªáu ƒë∆∞·ª£c l√†m s·∫°ch, c√°c ƒë·∫∑c tr∆∞ng bi·∫øn s·ªë li√™n t·ª•c nh∆∞ gi√°, kho·∫£ng gi√° th·∫•p nh·∫•t, l·ªõn nh·∫•t, tu·ªïi xe, s·ªë km ƒë√£ ƒëi ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ t·∫°o m√¥ h√¨nh ph√¢n c·ª•m')

    st.markdown('#### 2. Ph√¢n c·ª•m b·∫±ng c√°c ph∆∞∆°ng ph√°p kh√°c nhau')
    st.write('''
    M√¥ h√¨nh ph√¢n c·ª•m ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n 02 m√¥i tr∆∞·ªùng: m√°y h·ªçc truy·ªÅn th·ªëng (sci-kit learn) v√† PySpark.
    - M√°y h·ªçc truy·ªÅn th·ªëng: KMeans, Bisect Kmeans, Agglomerative clustering
    - PySpark: Kmeans, Bisecting Kmeans, GMM.

    ''')

    st.markdown('#### 3. K·∫øt qu·∫£')


    st.markdown('''
    S·ªë c·ª•m ƒë∆∞·ª£c t·∫°o th√†nh tr√™n m√¥ h√¨nh m√°y h·ªçc truy·ªÅn th·ªëng: **03 c·ª•m**
    S·ªë c·ª•m ƒë∆∞·ª£c t·∫°o th√†nh tr√™n PySpark: **02 c·ª•m**''')
    st.image("silhoutte_sklearn.png")                

    st.markdown('''      
    KMeans tr√™n m√¥i tr∆∞·ªùng m√°y h·ªçc truy·ªÅn th·ªëng cho k·∫øt qu·∫£ silhoutte score cao nh·∫•t v√† k·∫øt qu·∫£ ph√¢n c·ª•m d·ªÖ di·ªÖn gi·∫£i h∆°n.
    
    **Ph√¢n lo·∫°i ph√¢n kh√∫c xe**:                
    1/ C·ª•m 0: Ph√¢n kh√∫c Xe Ph·ªï Th√¥ng ‚Äì Trung c·∫•p (Mid-range Popular Motorcycles): Xe tu·ªïi trung b√¨nh, gi√° v·ª´a ph·∫£i, ph√π h·ª£p ƒë·∫°i ƒëa s·ªë ng∆∞·ªùi mua.   
    2/ C·ª•m 1: Ph√¢n kh√∫c Xe Cao C·∫•p ‚Äì Premium / High-end Motorcycles: Ti√™u bi·ªÉu l√† c√°c d√≤ng SH, Vespa cao c·∫•p, ph√¢n kh·ªëi l·ªõn, xe m·ªõi ch·∫°y √≠t.          
    3/ C·ª•m 2: Ph√¢n kh√∫c Xe C≈© ‚Äì Ti·∫øt Ki·ªám (Budget Used Motorcycles): Gi√° r·∫ª nh·∫•t, xe tu·ªïi cao, ch·∫°y nhi·ªÅu ‚Äî ph√π h·ª£p kh√°ch c·∫ßn xe r·∫ª ƒë·ªÉ di chuy·ªÉn c∆° b·∫£n.
    ''')


    st.write('''Trong 3 m√¥ h√¨nh ph√¢n c·ª•m KMeans, Bisect KMeans v√† Agglomerate th√¨ KMeans v·ªõi k = 3 cho k·∫øt qu·∫£ ph√¢n c·ª•m t·ªët nh·∫•t.
            n√™n m√¥ h√¨nh ph√¢n c·ª•m xe ƒë∆∞·ª£c s·ª≠ d·ª•ng trong ·ª©ng d·ª•ng n√†y l√† KMeans v·ªõi k = 3.''')

    st.markdown('#### 4. Th·ªëng k√™ theo t·ª´ng c·ª•m:')

    st.write('Tr·ª±c quan h√≥a')
    st.image('pca_clusters.png')

    cluster_summary = (
        df_cluster.groupby('cluster_label')
        .agg(
            count=('cluster_label', 'size'),
            avg_price=('price', 'mean'),
            avg_age=('age', 'mean'),
            avg_mileage=('mileage_km', 'mean')
        )
        .sort_values('cluster_label')
    )


    # Rename the index (cluster_label ‚Üí Nh√£n c·ª•m xe)
    cluster_summary = cluster_summary.rename_axis("Nh√£n c·ª•m xe")

    # Rename columns
    cluster_summary = cluster_summary.rename(columns={
        "count": "S·ªë l∆∞·ª£ng (xe)",
        "avg_price": "Gi√° trung b√¨nh (VND)",
        "avg_age": "Tu·ªïi trung b√¨nh (nƒÉm)",
        "avg_mileage": "S·ªë km trung b√¨nh (km)"
    })

    # Format s·ªë nguy√™n v√† th√™m d·∫•u ph·∫©y
    cluster_summary["Gi√° trung b√¨nh (VND)"] = (
        cluster_summary["Gi√° trung b√¨nh (VND)"]
            .round(0).astype(int)
            .map(lambda x: f"{x:,}")
    )

    cluster_summary["S·ªë km trung b√¨nh (km)"] = (
        cluster_summary["S·ªë km trung b√¨nh (km)"]
            .round(0).astype(int)
            .map(lambda x: f"{x:,}")
    )

    st.dataframe(cluster_summary, width='stretch')


elif page == "G·ª£i √Ω m·∫´u xe t∆∞∆°ng t·ª±":
    # Main page header
    st.markdown("""
    <h1 style='font-size:48px; font-weight:800; margin-bottom:8px;'>
        G·ª£i √Ω m·∫´u xe t∆∞∆°ng t·ª±
    </h1>
    <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
    """, unsafe_allow_html=True)

    # Prepare data + vector
    df_clean, X_final = df_clean, X_final

    # Styling and helpers
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)

    st.markdown("""
        <style>
        .card {
            border-radius: 10px;
            padding: 14px 16px;
            margin: 8px 0;
            border: 1px solid #eee;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            background-color: #ffffff;
        }
        .bike-title {
            font-size: 16px;
            font-weight: 700;
            margin-bottom: 4px;
        }
        .bike-sub {
            font-size: 13px;
            color: #666666;
        }
        .small-muted {
            font-size: 12px;
            color: #777777;
        }
        </style>
    """, unsafe_allow_html=True)

    def display_bike_card(row):
        title = row.get('title', 'N/A')
        price = fmt_vnd(row.get('price', None))
        brand = row.get('brand', '-')
        model = row.get('model', '-')
        km = row.get('mileage_km', '-')
        year = row.get('registration_year', '-')
        year_shown = int(year) if str(year).isdigit() else year
        origin = row.get('origin', '-')
        desc = row.get('description', '')

        card_html = f"""
        <div class='card'>
            <div style='display:flex; gap:14px; align-items:center'>
                <div style='flex:1'>
                    <div class='bike-title'>{title}</div>
                    <div class='bike-sub'>{brand} ‚Äî {model} ‚Ä¢ {origin}</div>
                    <div style='margin-top:6px'>{textwrap.shorten(str(desc), width=220)}</div>
                </div>
                <div style='text-align:right; min-width:150px'>
                    <div style='font-weight:700; font-size:16px'>{price}</div>
                    <div class='small-muted' style='margin-top:8px'>
                        S·ªë km: {km}<br/>NƒÉm: {year_shown}
                    </div>
                </div>
            </div>
        </div>
        """
        st.markdown(card_html, unsafe_allow_html=True)

    # ‚úÖ Main interaction
    yellow_pad_header("G·ª£i √Ω theo m·∫´u c√≥ s·∫µn")

    titles_list = df_clean['title'].unique().tolist()
    selected = st.selectbox("Ch·ªçn 1 m·∫´u trong danh s√°ch", titles_list)

    if st.button("G·ª£i √Ω"):
        with st.spinner("üîé ƒêang t√¨m m·∫´u t∆∞∆°ng t·ª±..."):
            df_top, scores = get_top_n_similar_by_content(
                df_clean,
                X_final,
                title=selected,
                top_n=5
            )

        if df_top is None or len(df_top) == 0:
            st.warning("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ‚Äî ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
        else:
            st.success(f"ƒê√£ t√¨m {len(df_top)} m·∫´u t∆∞∆°ng t·ª± ‚úÖ")

            # ‚úÖ Show selected bike
            st.markdown("#### üî∂ M·∫´u b·∫°n ƒë√£ ch·ªçn")
            selected_row = df_clean[df_clean["title"] == selected].iloc[0]
            display_bike_card(selected_row)

            # ‚úÖ Show recommendations
            st.markdown("#### üî∂ C√°c m·∫´u t∆∞∆°ng t·ª±")
            for _, row in df_top.iterrows():
                display_bike_card(row)
                st.caption(f"Similarity score: {row['similarity_score']:.3f}")

        
    # theo t·ª´ kh√≥a
    yellow_pad_header("T√¨m ki·∫øm theo t·ª´ kh√≥a")

    q = st.text_input('Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm, v√≠ d·ª•: "honda vision 2014 m√†u ƒë·ªè"')
    top_k = st.selectbox('S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ', [1, 3, 5, 10])

    if st.button('T√¨m ki·∫øm') and q.strip():
        with st.spinner('ƒêang x·ª≠ l√Ω t·ª´ kh√≥a...'):

            # 1) Clean query like training data
            q_clean = clean_text(q)

            # 2) Vectorize cleaned query
            q_vec_tfidf = vectorizer.transform([q_clean])

            # 3) Pad numeric features with zeros
            num_dim = X_final.shape[1] - q_vec_tfidf.shape[1]
            q_num_zeros = np.zeros((1, num_dim))

            # 4) Combine TF-IDF + numeric zeros
            q_vec = hstack([q_vec_tfidf, q_num_zeros])

            # 5) Compute similarity
            sim_scores = cosine_similarity(q_vec, X_final).flatten()

            # 6) Select top results
            idxs = sim_scores.argsort()[::-1][:top_k]

            # 7) Select rows from cleaned DF
            res_df = df_clean.iloc[idxs].copy()
            res_df['similarity_score'] = sim_scores[idxs]

        st.success(f'K·∫øt qu·∫£ top {top_k} cho: "{q}"')

        # 8) Display
        for _, row in res_df.iterrows():
            display_bike_card(row)
            st.caption(f"Similarity score: {row['similarity_score']:.3f}")


elif page == "Ph√¢n c·ª•m ph√¢n kh√∫c xe m√°y":
    # Main page header
    st.markdown("""
    <h1 style='font-size:48px; font-weight:800; margin-bottom:8px;'>
        Ph√¢n c·ª•m ph√¢n kh√∫c xe m√°y
    </h1>
    <div style='width:90px; height:6px; background:#FF9A00; border-radius:3px; margin-bottom:24px;'></div>
    """, unsafe_allow_html=True)

    # Yellow pad header function (keep for consistent style)
    def yellow_pad_header(text):
        st.markdown(f"""
            <div style="
                background: #FFF4C2;
                border-left: 6px solid #FFDE37;
                padding: 12px 18px;
                border-radius: 6px;
                font-size: 24px;
                font-weight: bold;
                color: #333;
                margin: 15px 0 10px 0;
            ">
                {text}
            </div>
        """, unsafe_allow_html=True)

    # ----- Card CSS -----
    st.markdown("""
        <style>
        .card {
            border-radius: 10px;
            padding: 14px 16px;
            margin: 8px 0;
            border: 1px solid #eee;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            background-color: #ffffff;
        }
        .bike-title {
            font-size: 16px;
            font-weight: 700;
            margin-bottom: 4px;
        }
        .bike-sub {
            font-size: 13px;
            color: #666666;
        }
        .small-muted {
            font-size: 12px;
            color: #777777;
        }
        </style>
    """, unsafe_allow_html=True)

    
    # ----- Main interaction -----
    yellow_pad_header("Ph√¢n c·ª•m xe m·ªõi")


    st.markdown("""
    <style>
    .cluster-card {
        padding: 15px;
        border-radius: 12px;
        margin-top: 10px;
        margin-bottom: 15px;
        color: white;
        font-size: 16px;
    }
    .cluster-0 {
        background: linear-gradient(135deg, #4CAF50, #2E7D32);
    }
    .cluster-1 {
        background: linear-gradient(135deg, #1976D2, #0D47A1);
    }
    .cluster-2 {
        background: linear-gradient(135deg, #F57C00, #E65100);
    }
    .cluster-title {
        font-size: 20px;
        font-weight: 700;
        margin-bottom: 5px;
    }
    .cluster-desc {
        font-size: 15px;
    }
    </style>
    """, unsafe_allow_html=True)


    # # st.markdown("""
    # # - **C·ª•m 0:** Xe ph·ªï th√¥ng ‚Äì gi√° r·∫ª, tu·ªïi xe trung b√¨nh, s·ªë km trung b√¨nh ‚Üí **nh√≥m chi·∫øm th·ªã ph·∫ßn l·ªõn nh·∫•t**.
    # # - **C·ª•m 1:** Xe m·ªõi h∆°n ‚Äì gi√° cao h∆°n, ch·∫°y √≠t h∆°n ‚Üí **ph√¢n kh√∫c ch·∫•t l∆∞·ª£ng t·ªët**.
    # # - **C·ª•m 2:** Xe r·∫•t c≈© ‚Äì gi√° th·∫•p nh·∫•t, s·ªë km c·ª±c cao ‚Üí **ph√¢n kh√∫c xu·ªëng c·∫•p ho·∫∑c d·ªØ li·ªáu km kh√¥ng ch√≠nh x√°c**.
    # # """)

    # bike_labels = {0: "Xe ph·ªï th√¥ng gi√° r·∫ª, tu·ªïi xe trung b√¨nh",
    #                1: "Xe t∆∞∆°ng ƒë·ªëi m·ªõi, ph√¢n kh√∫c cao c·∫•p",
    #                2: "Xe c≈© xu·ªëng c·∫•p ho·∫∑c d·ªØ li·ªáu cung c·∫•p kh√¥ng ch√≠nh x√°c"}


    # ====== CLUSTER NEW BIKE ======
    st.write("Vui l√≤ng nh·∫≠p c√°c th√¥ng s·ªë c·ªßa xe c·∫ßn x√°c ƒë·ªãnh")

    col1, col2 = st.columns(2)

    with col1:
        price = st.number_input("Gi√° xe (VND)", min_value=500_000, step=100_000, value=1_000_000)
        min_price = st.number_input("Kho·∫£ng gi√° min", min_value=500_000, step=100_000, value=800_000)

    with col2:
        max_price = st.number_input("Kho·∫£ng gi√° max", min_value=500_000, step=100_000, value=1_200_000)
        mileage_km = st.number_input("S·ªë km ƒë√£ ƒëi", min_value=0, step=100, value=1000)

    registration_year = st.slider("NƒÉm ƒëƒÉng k√Ω", 1980, 2025)

    if st.button("Ph√¢n c·ª•m"):
        X_new = preprocess_user_input(price, min_price, max_price, mileage_km, registration_year)
        cluster = int(kmeans.predict(X_new)[0])
        st.success(f"Xe thu·ªôc c·ª•m s·ªë **{cluster}**")

        # st.write(bike_labels.get(cluster, "Kh√¥ng c√≥ m√¥ t·∫£ cho c·ª•m n√†y"))

        # ======= HI·ªÇN TH·ªä TH·∫∫ GI·∫¢I TH√çCH C·ª§M THEO K·∫æT QU·∫¢ =======

        cluster_cards = {
            0: """
                <div class="cluster-card cluster-0">
                    <div class="cluster-title">C·ª•m 0 ‚Äì Xe ph·ªï th√¥ng gi√° r·∫ª</div>
                    <div class="cluster-desc">
                        Gi√° th·∫•p ‚Äì tu·ªïi xe trung b√¨nh ‚Äì s·ªë km ch·∫°y v·ª´a ph·∫£i.<br>
                        Ph√¢n kh√∫c xe ph·ªï th√¥ng, ph√π h·ª£p ƒëa s·ªë ng∆∞·ªùi mua.
                    </div>
                </div>
            """,
            1: """
                <div class="cluster-card cluster-1">
                    <div class="cluster-title">C·ª•m 1 ‚Äì Xe cao c·∫•p / √≠t ch·∫°y</div>
                    <div class="cluster-desc">
                        Xe m·ªõi ‚Äì √≠t km ‚Äì gi√° cao.<br>
                        C√°c d√≤ng SH, Vespa, xe cao c·∫•p, t√¨nh tr·∫°ng t·ªët.
                    </div>
                </div>
            """,
            2: """
                <div class="cluster-card cluster-2">
                    <div class="cluster-title">C·ª•m 2 ‚Äì Xe c≈© / gi√° r·∫ª</div>
                    <div class="cluster-desc">
                        Gi√° th·∫•p nh·∫•t ‚Äì km r·∫•t cao ‚Äì tu·ªïi xe l·ªõn.<br>
                        Ph√¢n kh√∫c xe ƒë√£ c≈© ho·∫∑c c√≥ d·∫•u hi·ªáu xu·ªëng c·∫•p.
                    </div>
                </div>
            """
                }
        st.markdown("""
        <style>
        .cluster-card {
            border-radius: 10px;
            padding: 14px 18px;
            margin: 10px 0;
            border: 1px solid #E5C600;
            box-shadow: 0 2px 4px rgba(0,0,0,0.08);
            color: #000000;
        }

        .cluster-title {
            font-weight: 700;
            font-size: 18px;
            margin-bottom: 6px;
            color: #000000;
        }

        .cluster-desc {
            font-size: 14px;
            color: #000000;
            line-height: 1.4;
        }

        /* ‚úÖ Different yellow for each cluster */
        .cluster-0 { background: #FFF7A6; }
        .cluster-1 { background: #FFE970; }
        .cluster-2 { background: #FFDE37; }
        </style>
        """, unsafe_allow_html=True)

        # Hi·ªÉn th·ªã card t∆∞∆°ng ·ª©ng
        st.markdown(cluster_cards.get(cluster, ""), unsafe_allow_html=True)

